{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo State Stochastic Volatility Heston Dynamical Model\n",
    "\n",
    "# The Modeling Perspective\n",
    "\n",
    "This data is from SPX&500. Here, we're interested in modeling the stock price rather than modeling the option price, as of calculating option requires us model the stock price first. We know about the stock price follows the Heston Model:\n",
    "\n",
    "\\begin{aligned}\n",
    "dS_t &= \\mu S_t dt + \\sqrt{v_t}S_td\\tilde W_t \\\\\n",
    "dv_t &= k(\\mu - v_t)dt + \\alpha \\sqrt{v_t}d\\tilde W_t'\\\\\n",
    "\\rho dt &= \\mathbb{E}(d\\tilde W_t+d\\tilde W_t')\n",
    "\\end{aligned}\n",
    "\n",
    "This does have a close solution by geometrical transformation. However, here we're using the Echo State Neural Network to model the volatility process $v_t$. Therefore, the heston model becomes:\n",
    "\n",
    "\\begin{aligned}\n",
    "dS_t &= \\mu S_t dt + \\sqrt{v_t}S_td\\tilde W_t \\\\\n",
    "v_t &= \\text{ESN}_t(\\mathbf{u}(t), \\mathbf{x}(t)) \\\\\n",
    "\\mathbf{u}(t) &= \\text{return}(:t-1) \\\\\n",
    "\\mathbf{x}(t) &= \\sigma_{\\text{tanh}} ( \\mathbf{W}_{\\text{in}}\\mathbf{u}(t) + \\mathbf{W}_{\\text{r}}(\\mathbf{x}(t-1) + \\epsilon) )\n",
    "\\end{aligned}\n",
    "\n",
    "We might first model the constant drift geometric brownian motion process. Then, we can build another parallel ESN for forcasting the stochastic drift process, namely: \n",
    "\n",
    "\\begin{aligned}\n",
    "dS_t &= \\mu_t S_t dt + \\sqrt{v_t}S_td\\tilde W_t \\\\\n",
    "v_t &= \\text{ESN}^1_t(\\mathbf{u}(t), \\mathbf{x}(t)) \\\\\n",
    "\\mu_t &= \\text{ESN}^2_t(\\mathbf{u}(t), \\mathbf{x}(t))\\\\\n",
    "\\mathbf{u}(t) &= \\text{return}(:t-1) \\\\\n",
    "\\mathbf{x}(t) &= \\sigma_{\\text{tanh}} ( \\mathbf{W}_{\\text{in}}\\mathbf{u}(t) + \\mathbf{W}_{\\text{r}}(\\mathbf{x}(t-1) + \\epsilon) )\n",
    "\\end{aligned}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def EchoStateDeepTest(x_dim, connectivity, spectral_radius,\n",
    "                  u_num, u_dim, u_mag,\n",
    "                  leak, cutout, forget, \n",
    "                  cv_start, cv_end, cv_step, val_cut, verbose, input_U, target_Y, resvoir_num):\n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"-----------------------------------------------Making Transition and input matrix------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "    # Making inner transition sparse matrix W\n",
    "    W_list = []\n",
    "    for i in range(resvoir_num):\n",
    "        nans = np.random.randint(0, int(1/connectivity)-1 , size=(x_dim, x_dim))\n",
    "        W = np.random.uniform(-0.4, 0.4, x_dim*x_dim).reshape([x_dim, x_dim])\n",
    "        W = np.where(nans, np.nan, W)\n",
    "        W = np.nan_to_num(W)\n",
    "        E, _ = np.linalg.eig(W)\n",
    "        e_max = np.max(np.abs(E))\n",
    "        W /= np.abs(e_max)/spectral_radius   \n",
    "        W_list.append(W)\n",
    "\n",
    "    # Making reservoir to reservoir transitional matrix \n",
    "    W_trans_list = []\n",
    "    for i in range(resvoir_num-1):\n",
    "        W_trans = np.random.uniform(-1, 1, x_dim*x_dim).reshape([x_dim, x_dim])\n",
    "        W_trans_list.append(W_trans) \n",
    "\n",
    "\n",
    "    # Making input matrix W_in\n",
    "    W_in = np.random.uniform(-1, 1, x_dim*u_dim).reshape([x_dim, u_dim])\n",
    "    W_in = W_in / (np.linalg.svd(W_in)[1].tolist()[0]*1.2)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Shape of W: \", str(W.shape))\n",
    "        print(\"Shape of W_in:\", str(W_in.shape))\n",
    "\n",
    "        print()\n",
    "        print(\"-----------------------------------------------Making input and inner states------------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Making Inner States\n",
    "    X = u\n",
    "\n",
    "    x_lists = []\n",
    "    x_list = [np.zeros([x_dim]), np.zeros([x_dim])]\n",
    "\n",
    "    # 1st reservoir\n",
    "    for i in range(u_num):\n",
    "        x_next = (1-leak)*x_list[-2] + leak * np.tanh( np.matmul(W_in, u[i]) + np.matmul(W_list[0], x_list[-1] ) +  np.random.rand(x_dim))\n",
    "        x_list.append(x_next)\n",
    "    x_lists.append(x_list)\n",
    "\n",
    "    # 2-last reservoirs\n",
    "    for res in range(resvoir_num-1):\n",
    "        x_list = [np.zeros([x_dim]), np.zeros([x_dim])]\n",
    "        for i in range(u_num):\n",
    "            x_next = (1-leak)*x_list[-2] + leak * np.tanh( np.matmul(W_trans_list[res], x_lists[res][i+2]) + np.matmul(W_list[res], x_list[-1] ) +  np.random.rand(x_dim))\n",
    "            x_list.append(x_next)\n",
    "        x_lists.append(x_list)\n",
    "\n",
    "    states = np.array(x_lists[-1][1:]).reshape(u_num+1, x_dim)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Inner States: # of samples x # of dimension:\", str(states.shape))\n",
    "        print(\"Input States: # of samples x # of dimension:\", str(u.shape))\n",
    "\n",
    "        print()\n",
    "        print(\"------------------------------------------------Concatenate data and Y sequence data------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "        # Making Concatenated data\n",
    "\n",
    "    X = np.concatenate([states[:-1,:], u], axis=1)\n",
    "    X = X[:-1, :]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Faking Target sequence\n",
    "    Y = target_Y\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Inner + Input States: # of samples x # of dimension:\", str(X.shape))\n",
    "        print(\"Targeted fitting sequence: # of samples x # of dimension:\", str(Y.shape))\n",
    "\n",
    "        print()\n",
    "        print(\"--------------------------------------------------Splitting Data to 3-------------------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "    # Split into 3 trunks, usless trunk, regressing trunk, predicting trunk\n",
    "    useless_X = X[:forget, :]\n",
    "    useless_Y = Y[:forget, :]\n",
    "\n",
    "    regress_X = X[forget:-cutout, :]\n",
    "    regress_Y = Y[forget:-cutout, :]\n",
    "\n",
    "    train_size = int(regress_X.shape[0]*(1-val_cut))\n",
    "\n",
    "    train_X = np.split(regress_X, [train_size, regress_X.shape[0]+1])[0]\n",
    "    train_Y = np.split(regress_Y, [train_size, regress_X.shape[0]+1])[0]\n",
    "\n",
    "    val_X = np.split(regress_X, [train_size, regress_X.shape[0]+1])[1]\n",
    "    val_Y = np.split(regress_Y, [train_size, regress_X.shape[0]+1])[1]\n",
    "\n",
    "    predict_X = X[-cutout:, :]\n",
    "    predict_Y = Y[-cutout:, :]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"useless_X: # of samples x # of dimension:\", str(useless_X.shape))\n",
    "        print(\"useless_Y: # of samples x # of dimension:\", str(useless_Y.shape))\n",
    "        print(\"regress_X: # of samples x # of dimension:\", str(regress_X.shape))\n",
    "        print(\"regress_Y: # of samples x # of dimension:\", str(regress_Y.shape))\n",
    "        print(\"train_X: # of samples x # of dimension:\", str(train_X.shape))\n",
    "        print(\"train_Y: # of samples x # of dimension:\", str(train_Y.shape))\n",
    "        print(\"val_X: # of samples x # of dimension:\", str(val_X.shape))\n",
    "        print(\"val_Y: # of samples x # of dimension:\", str(val_Y.shape))\n",
    "        print(\"predict_X: # of samples x # of dimension:\", str(predict_X.shape))\n",
    "        print(\"predict_Y: # of samples x # of dimension:\", str(predict_Y.shape))\n",
    "\n",
    "        print()\n",
    "        print(\"---------------------------------------------------Conducting Regression----------------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "    alpha = cv_start\n",
    "    mse = {}\n",
    "    while alpha <= cv_end:\n",
    "\n",
    "        # Conducting linear regression\n",
    "        reg = Ridge(alpha).fit(train_X, train_Y)\n",
    "\n",
    "        # Making prediction\n",
    "        valhat_Y = reg.predict(val_X)\n",
    "        alpha += cv_step\n",
    "\n",
    "        loss = np.mean(np.multiply(   (val_Y - valhat_Y), (val_Y - valhat_Y)))\n",
    "        mse[alpha] = loss\n",
    "\n",
    "\n",
    "    best_mse = min(list(mse.values()))\n",
    "    best_alpha = list(mse.keys())[list(mse.values()).index(best_mse)]\n",
    "\n",
    "\n",
    "    # using best regression again\n",
    "    reg = Ridge(best_alpha).fit(regress_X, regress_Y)\n",
    "\n",
    "    # Making prediction\n",
    "    predhat_Y = reg.predict(predict_X)\n",
    "\n",
    "    # showing training error, \n",
    "    regrpred_Y = reg.predict(regress_X)\n",
    "    train_mse = np.mean(np.multiply(   (regrpred_Y - regress_Y), (regrpred_Y - regress_Y)))\n",
    "    pred_mse = np.mean(np.multiply(   (predhat_Y - predict_Y), (predhat_Y - predict_Y)))\n",
    "\n",
    "    print(\"model ridge coefficient:\", best_alpha)\n",
    "    print(\"Model training mse:\", train_mse)\n",
    "    print(\"Model validation mse:\", best_mse)\n",
    "    print(\"Model prediction mse:\", pred_mse)\n",
    "    print(\"Model prediction average error\", math.sqrt(pred_mse))\n",
    "\n",
    "    if verbose:    \n",
    "        print(\"regress coefficient length:\", len(reg.coef_[0].tolist()))\n",
    "        print(\"first 5 coefficient of model:\", reg.coef_[0, :5])\n",
    "        print(\"Predicted length equal to target length:\", predhat_Y.shape==predict_Y.shape)\n",
    "\n",
    "        print()\n",
    "        print(\"-----------------------------------------------Producing Graphic Visualization----------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "\n",
    "    # Producing Graphic Visualization\n",
    "\n",
    "    # 后面100个，真实值与预测的\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    ax.set_title('Predict and Groud Truth'.format(\"seaborn\"), color='C0')   \n",
    "\n",
    "    ax.plot([j for j in range(cutout)], [predict_Y[j] for j in range(predict_Y.shape[0])])\n",
    "    ax.plot([j for j in range(cutout)], [predhat_Y[j] for j in range(predhat_Y.shape[0])], \"--\")\n",
    "\n",
    "\n",
    "    # 所有的，真实值与预测的\n",
    "    hat_Y = reg.predict(X)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    ax.set_title('Predict and Groud Truth everything'.format(\"seaborn\"), color='C1')   \n",
    "\n",
    "    ax.plot([j for j in range(forget, u_num-1)], [Y[j] for j in range(forget, u_num-1)], \":\", alpha = 0.7)\n",
    "    ax.plot([j for j in range(forget, u_num-1)], [hat_Y[j] for j in range(forget, u_num-1)], \"red\", linewidth=1)\n",
    "    ax.axvline(x=forget, ls = \"--\", c = \"yellow\")\n",
    "    ax.axvline(x=u_num - cutout, ls = \"--\", c = \"yellow\")\n",
    "\n",
    "\n",
    "    # Different in prediction\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    ax.set_title('Predict and Groud Truth everything'.format(\"seaborn\"), color='C1')   \n",
    "\n",
    "    ax.plot([j for j in range(forget, u_num-1)], [Y[j]-hat_Y[j] for j in range(forget, u_num-1)], \"black\", linewidth=1)\n",
    "    ax.axvline(x=forget, ls = \"--\", c = \"yellow\")\n",
    "    ax.axvline(x=u_num - cutout, ls = \"--\", c = \"yellow\")\n",
    "\n",
    "\n",
    "    # all predictor signals\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    ax.set_title('all predictor signals'.format(\"seaborn\"), color='C1')   \n",
    "    for i in range(1):\n",
    "        ax.plot([j for j in range(1, u_num-1)], [X[j, i] for j in range(1, u_num-1)], linewidth=0.5)\n",
    "    for i in range(-1,-6, -1):\n",
    "        ax.plot([j for j in range(1, u_num-1)], [X[j, i] for j in range(1, u_num-1)], linewidth=0.5)\n",
    "    ax.axvline(x=forget, ls = \"--\", c = \"yellow\")\n",
    "    ax.axvline(x=u_num - cutout, ls = \"--\", c = \"yellow\")\n",
    "\n",
    "    return predict_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EchoStateCovTest(x_dim, connectivity, spectral_radius,\n",
    "                  u_num, u_dim, u_mag,\n",
    "                  leak, cutout, forget, \n",
    "                  cv_start, cv_end, cv_step, val_cut, verbose, input_U, target_Y, resvoir_num):\n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"-----------------------------------------------Making Transition and input matrix------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "    # Making inner transition sparse matrix W\n",
    "    W_list = []\n",
    "    for i in range(resvoir_num):\n",
    "        nans = np.random.randint(0, int(1/connectivity)-1 , size=(x_dim, x_dim))\n",
    "        W = np.random.uniform(-0.4, 0.4, x_dim*x_dim).reshape([x_dim, x_dim])\n",
    "        W = np.where(nans, np.nan, W)\n",
    "        W = np.nan_to_num(W)\n",
    "        E, _ = np.linalg.eig(W)\n",
    "        e_max = np.max(np.abs(E))\n",
    "        W /= np.abs(e_max)/spectral_radius   \n",
    "        W_list.append(W)\n",
    "\n",
    "    # Making input matrix W_in\n",
    "    W_in = np.random.uniform(-1, 1, x_dim*u_dim).reshape([x_dim, u_dim])\n",
    "    W_in = W_in / (np.linalg.svd(W_in)[1].tolist()[0]*1.2)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Shape of W: \", str(W.shape))\n",
    "        print(\"Shape of W_in:\", str(W_in.shape))\n",
    "\n",
    "        print()\n",
    "        print(\"-----------------------------------------------Making input and inner states------------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "    # Making Inner States\n",
    "    X = u\n",
    "    \n",
    "    for res in range(resvoir_num):\n",
    "        x_list = [np.zeros([x_dim]), np.zeros([x_dim])]\n",
    "\n",
    "        for i in range(u_num):\n",
    "            x_next = (1-leak)*x_list[-2] + leak * np.tanh( np.matmul(W_in, u[i]) + np.matmul(W_list[res], x_list[-1] ) +  np.random.rand(x_dim))\n",
    "            x_list.append(x_next)\n",
    "\n",
    "        states = np.array(x_list[1:]).reshape(u_num+1, x_dim)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Inner States: # of samples x # of dimension:\", str(states.shape))\n",
    "            print(\"Input States: # of samples x # of dimension:\", str(u.shape))\n",
    "\n",
    "            print()\n",
    "            print(\"------------------------------------------------Concatenate data and Y sequence data------------------------------------------------\")\n",
    "            print()\n",
    "\n",
    "        # Making Concatenated data\n",
    "\n",
    "        X = np.concatenate([states[:-1,:], u], axis=1)\n",
    "    X = X[:-1, :]\n",
    "\n",
    "    # Faking Target sequence\n",
    "    Y = target_Y\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Inner + Input States: # of samples x # of dimension:\", str(X.shape))\n",
    "        print(\"Targeted fitting sequence: # of samples x # of dimension:\", str(Y.shape))\n",
    "\n",
    "        print()\n",
    "        print(\"--------------------------------------------------Splitting Data to 3-------------------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "    # Split into 3 trunks, usless trunk, regressing trunk, predicting trunk\n",
    "    useless_X = X[:forget, :]\n",
    "    useless_Y = Y[:forget, :]\n",
    "\n",
    "    regress_X = X[forget:-cutout, :]\n",
    "    regress_Y = Y[forget:-cutout, :]\n",
    "    \n",
    "    train_size = int(regress_X.shape[0]*(1-val_cut))\n",
    "    \n",
    "    train_X = np.split(regress_X, [train_size, regress_X.shape[0]+1])[0]\n",
    "    train_Y = np.split(regress_Y, [train_size, regress_X.shape[0]+1])[0]\n",
    "       \n",
    "    val_X = np.split(regress_X, [train_size, regress_X.shape[0]+1])[1]\n",
    "    val_Y = np.split(regress_Y, [train_size, regress_X.shape[0]+1])[1]\n",
    "        \n",
    "    predict_X = X[-cutout:, :]\n",
    "    predict_Y = Y[-cutout:, :]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"useless_X: # of samples x # of dimension:\", str(useless_X.shape))\n",
    "        print(\"useless_Y: # of samples x # of dimension:\", str(useless_Y.shape))\n",
    "        print(\"regress_X: # of samples x # of dimension:\", str(regress_X.shape))\n",
    "        print(\"regress_Y: # of samples x # of dimension:\", str(regress_Y.shape))\n",
    "        print(\"train_X: # of samples x # of dimension:\", str(train_X.shape))\n",
    "        print(\"train_Y: # of samples x # of dimension:\", str(train_Y.shape))\n",
    "        print(\"val_X: # of samples x # of dimension:\", str(val_X.shape))\n",
    "        print(\"val_Y: # of samples x # of dimension:\", str(val_Y.shape))\n",
    "        print(\"predict_X: # of samples x # of dimension:\", str(predict_X.shape))\n",
    "        print(\"predict_Y: # of samples x # of dimension:\", str(predict_Y.shape))\n",
    "\n",
    "        print()\n",
    "        print(\"---------------------------------------------------Conducting Regression----------------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "    alpha = cv_start\n",
    "    mse = {}\n",
    "    while alpha <= cv_end:\n",
    "\n",
    "        # Conducting linear regression\n",
    "        reg = Ridge(alpha).fit(train_X, train_Y)\n",
    "\n",
    "        # Making prediction\n",
    "        valhat_Y = reg.predict(val_X)\n",
    "        alpha += cv_step\n",
    "\n",
    "        loss = np.mean(np.multiply(   (val_Y - valhat_Y), (val_Y - valhat_Y)))\n",
    "        mse[alpha] = loss\n",
    "\n",
    "\n",
    "    best_mse = min(list(mse.values()))\n",
    "    best_alpha = list(mse.keys())[list(mse.values()).index(best_mse)]\n",
    "\n",
    "\n",
    "    # using best regression again\n",
    "    reg = Ridge(best_alpha).fit(regress_X, regress_Y)\n",
    "\n",
    "    # Making prediction\n",
    "    predhat_Y = reg.predict(predict_X)\n",
    "    \n",
    "    # showing training error, \n",
    "    regrpred_Y = reg.predict(regress_X)\n",
    "    train_mse = np.mean(np.multiply(   (regrpred_Y - regress_Y), (regrpred_Y - regress_Y)))\n",
    "    pred_mse = np.mean(np.multiply(   (predhat_Y - predict_Y), (predhat_Y - predict_Y)))\n",
    "    \n",
    "    print(\"model ridge coefficient:\", best_alpha)\n",
    "    print(\"Model training mse:\", train_mse)\n",
    "    print(\"Model validation mse:\", best_mse)\n",
    "    print(\"Model prediction mse:\", pred_mse)\n",
    "    print(\"Model prediction average error\", math.sqrt(pred_mse))\n",
    "\n",
    "    if verbose:    \n",
    "        print(\"regress coefficient length:\", len(reg.coef_[0].tolist()))\n",
    "        print(\"first 5 coefficient of model:\", reg.coef_[0, :5])\n",
    "        print(\"Predicted length equal to target length:\", predhat_Y.shape==predict_Y.shape)\n",
    "\n",
    "        print()\n",
    "        print(\"-----------------------------------------------Producing Graphic Visualization----------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "\n",
    "    # Producing Graphic Visualization\n",
    "\n",
    "    # 后面100个，真实值与预测的\n",
    "    fig, ax = plt.subplots(figsize=(20, 3))\n",
    "    ax.set_title('Predict and Groud Truth'.format(\"seaborn\"), color='C0')   \n",
    "\n",
    "    ax.plot([j for j in range(cutout)], [predict_Y[j] for j in range(predict_Y.shape[0])])\n",
    "    ax.plot([j for j in range(cutout)], [predhat_Y[j] for j in range(predhat_Y.shape[0])], \"--\")\n",
    "\n",
    "\n",
    "    # 所有的，真实值与预测的\n",
    "    hat_Y = reg.predict(X)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    ax.set_title('Predict and Groud Truth everything'.format(\"seaborn\"), color='C1')   \n",
    "\n",
    "    ax.plot([j for j in range(forget, u_num-1)], [Y[j] for j in range(forget, u_num-1)], \":\", alpha = 0.7)\n",
    "    ax.plot([j for j in range(forget, u_num-1)], [hat_Y[j] for j in range(forget, u_num-1)], \"red\", linewidth=1)\n",
    "    ax.axvline(x=forget, ls = \"--\", c = \"yellow\")\n",
    "    ax.axvline(x=u_num - cutout, ls = \"--\", c = \"yellow\")\n",
    "\n",
    "    \n",
    "    # Different in prediction\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    ax.set_title('Predict and Groud Truth everything'.format(\"seaborn\"), color='C1')   \n",
    "\n",
    "    ax.plot([j for j in range(forget, u_num-1)], [Y[j]-hat_Y[j] for j in range(forget, u_num-1)], \"black\", linewidth=1)\n",
    "    ax.axvline(x=forget, ls = \"--\", c = \"yellow\")\n",
    "    ax.axvline(x=u_num - cutout, ls = \"--\", c = \"yellow\")\n",
    "\n",
    "\n",
    "    # all predictor signals\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    ax.set_title('all predictor signals'.format(\"seaborn\"), color='C1')   \n",
    "    for i in range(1):\n",
    "        ax.plot([j for j in range(1, u_num-1)], [X[j, i] for j in range(1, u_num-1)], linewidth=0.5)\n",
    "    for i in range(-1,-6, -1):\n",
    "        ax.plot([j for j in range(1, u_num-1)], [X[j, i] for j in range(1, u_num-1)], linewidth=0.5)\n",
    "    ax.axvline(x=forget, ls = \"--\", c = \"yellow\")\n",
    "    ax.axvline(x=u_num - cutout, ls = \"--\", c = \"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EchoStateTest(x_dim, connectivity, spectral_radius,\n",
    "                  u_num, u_dim, u_mag,\n",
    "                  leak, cutout, forget, \n",
    "                  cv_start, cv_end, cv_step, val_cut, verbose, input_U, target_Y):\n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"-----------------------------------------------Making Transition and input matrix------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "    # Making inner transition sparse matrix W\n",
    "    nans = np.random.randint(0, int(1/connectivity)-1 , size=(x_dim, x_dim))\n",
    "    W = np.random.uniform(-0.4, 0.4, x_dim*x_dim).reshape([x_dim, x_dim])\n",
    "    W = np.where(nans, np.nan, W)\n",
    "    W = np.nan_to_num(W)\n",
    "    E, _ = np.linalg.eig(W)\n",
    "    e_max = np.max(np.abs(E))\n",
    "    W /= np.abs(e_max)/spectral_radius   \n",
    "\n",
    "\n",
    "    # Making input matrix W_in\n",
    "    W_in = np.random.uniform(-1, 1, x_dim*u_dim).reshape([x_dim, u_dim])\n",
    "    W_in = W_in / (np.linalg.svd(W_in)[1].tolist()[0]*1.2)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Shape of W: \", str(W.shape))\n",
    "        print(\"Shape of W_in:\", str(W_in.shape))\n",
    "\n",
    "        print()\n",
    "        print(\"-----------------------------------------------Making input and inner states------------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "    # Making Inner States\n",
    "    x_list = [np.zeros([x_dim]), np.zeros([x_dim])]\n",
    "    \n",
    "    for i in range(u_num):\n",
    "        x_next = (1-leak)*x_list[-2] + leak * np.tanh( np.matmul(W_in, u[i]) + np.matmul(W, x_list[-1] ) +  np.random.rand(x_dim))\n",
    "        x_list.append(x_next)\n",
    "\n",
    "    states = np.array(x_list[1:]).reshape(u_num+1, x_dim)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Inner States: # of samples x # of dimension:\", str(states.shape))\n",
    "        print(\"Input States: # of samples x # of dimension:\", str(u.shape))\n",
    "\n",
    "        print()\n",
    "        print(\"------------------------------------------------Concatenate data and Y sequence data------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "    # Making Concatenated data\n",
    "    \n",
    "    X = np.concatenate([states[:-1,:], u], axis=1)\n",
    "    X = X[:-1, :]\n",
    "\n",
    "    # Faking Target sequence\n",
    "    Y = target_Y\n",
    "    print(\"Y shape:\", Y.shape)\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Inner + Input States: # of samples x # of dimension:\", str(X.shape))\n",
    "        print(\"Targeted fitting sequence: # of samples x # of dimension:\", str(Y.shape))\n",
    "\n",
    "        print()\n",
    "        print(\"--------------------------------------------------Splitting Data to 3-------------------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "    # Split into 3 trunks, usless trunk, regressing trunk, predicting trunk\n",
    "    useless_X = X[:forget, :]\n",
    "    useless_Y = Y[:forget, :]\n",
    "\n",
    "    regress_X = X[forget:-cutout, :]\n",
    "    regress_Y = Y[forget:-cutout, :]\n",
    "    \n",
    "    train_size = int(regress_X.shape[0]*(1-val_cut))\n",
    "    \n",
    "    train_X = np.split(regress_X, [train_size, regress_X.shape[0]+1])[0]\n",
    "    train_Y = np.split(regress_Y, [train_size, regress_X.shape[0]+1])[0]\n",
    "       \n",
    "    val_X = np.split(regress_X, [train_size, regress_X.shape[0]+1])[1]\n",
    "    val_Y = np.split(regress_Y, [train_size, regress_X.shape[0]+1])[1]\n",
    "        \n",
    "    predict_X = X[-cutout:, :]\n",
    "    predict_Y = Y[-cutout:, :]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"useless_X: # of samples x # of dimension:\", str(useless_X.shape))\n",
    "        print(\"useless_Y: # of samples x # of dimension:\", str(useless_Y.shape))\n",
    "        print(\"regress_X: # of samples x # of dimension:\", str(regress_X.shape))\n",
    "        print(\"regress_Y: # of samples x # of dimension:\", str(regress_Y.shape))\n",
    "        print(\"train_X: # of samples x # of dimension:\", str(train_X.shape))\n",
    "        print(\"train_Y: # of samples x # of dimension:\", str(train_Y.shape))\n",
    "        print(\"val_X: # of samples x # of dimension:\", str(val_X.shape))\n",
    "        print(\"val_Y: # of samples x # of dimension:\", str(val_Y.shape))\n",
    "        print(\"predict_X: # of samples x # of dimension:\", str(predict_X.shape))\n",
    "        print(\"predict_Y: # of samples x # of dimension:\", str(predict_Y.shape))\n",
    "\n",
    "        print()\n",
    "        print(\"---------------------------------------------------Conducting Regression----------------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "    alpha = cv_start\n",
    "    mse = {}\n",
    "    while alpha <= cv_end:\n",
    "\n",
    "        # Conducting linear regression\n",
    "        reg = Ridge(alpha).fit(train_X, train_Y)\n",
    "\n",
    "        # Making prediction\n",
    "        valhat_Y = reg.predict(val_X)\n",
    "        alpha += cv_step\n",
    "\n",
    "        loss = np.mean(np.multiply(   (val_Y - valhat_Y), (val_Y - valhat_Y)))\n",
    "        mse[alpha] = loss\n",
    "\n",
    "\n",
    "    best_mse = min(list(mse.values()))\n",
    "    best_alpha = list(mse.keys())[list(mse.values()).index(best_mse)]\n",
    "\n",
    "\n",
    "    # using best regression again\n",
    "    reg = Ridge(best_alpha).fit(regress_X, regress_Y)\n",
    "\n",
    "    # Making prediction\n",
    "    predhat_Y = reg.predict(predict_X)\n",
    "    \n",
    "    # showing training error, \n",
    "    regrpred_Y = reg.predict(regress_X)\n",
    "    train_mse = np.mean(np.multiply(   (regrpred_Y - regress_Y), (regrpred_Y - regress_Y)))\n",
    "    pred_mse = np.mean(np.multiply(   (predhat_Y - predict_Y), (predhat_Y - predict_Y)))\n",
    "    \n",
    "    print(\"model ridge coefficient:\", best_alpha)\n",
    "    print(\"Model training mse:\", train_mse)\n",
    "    print(\"Model validation mse:\", best_mse)\n",
    "    print(\"Model prediction mse:\", pred_mse)\n",
    "    print(\"Model prediction average error\", math.sqrt(pred_mse))\n",
    "\n",
    "    if verbose:    \n",
    "        print(\"regress coefficient length:\", len(reg.coef_[0].tolist()))\n",
    "        print(\"first 5 coefficient of model:\", reg.coef_[0, :5])\n",
    "        print(\"Predicted length equal to target length:\", predhat_Y.shape==predict_Y.shape)\n",
    "\n",
    "        print()\n",
    "        print(\"-----------------------------------------------Producing Graphic Visualization----------------------------------------------------\")\n",
    "        print()\n",
    "\n",
    "\n",
    "    # Producing Graphic Visualization\n",
    "\n",
    "    # 后面100个，真实值与预测的\n",
    "    mpl.style.use(\"seaborn\")\n",
    "    fig, ax = plt.subplots(figsize=(20, 3))\n",
    "    ax.set_title('Predict and Groud Truth'.format(\"seaborn\"), color='C0')   \n",
    "\n",
    "    ax.plot([j for j in range(cutout)], [predict_Y[j] for j in range(predict_Y.shape[0])])\n",
    "    ax.plot([j for j in range(cutout)], [predhat_Y[j] for j in range(predhat_Y.shape[0])], \"--\")\n",
    "\n",
    "\n",
    "    # 所有的，真实值与预测的\n",
    "    hat_Y = reg.predict(X)\n",
    "\n",
    "\n",
    "    mpl.style.use(\"seaborn\")\n",
    "    fig, ax = plt.subplots(figsize=(20, 10))\n",
    "    ax.set_title('Predict and Groud Truth everything'.format(\"seaborn\"), color='C1')   \n",
    "\n",
    "    ax.plot([j for j in range(forget, u_num-1)], [Y[j] for j in range(forget, u_num-1)], \":\", alpha = 0.7)\n",
    "    ax.plot([j for j in range(forget, u_num-1)], [hat_Y[j] for j in range(forget, u_num-1)], \"red\", linewidth=1)\n",
    "    ax.plot([j for j in range(forget, u_num-1)], [Y[j]-hat_Y[j] for j in range(forget, u_num-1)], \"black\", linewidth=1)\n",
    "    ax.axvline(x=forget, ls = \"--\", c = \"yellow\")\n",
    "    ax.axvline(x=u_num - cutout, ls = \"--\", c = \"yellow\")\n",
    "\n",
    "\n",
    "    # all predictor signals\n",
    "    mpl.style.use(\"seaborn\")\n",
    "    fig, ax = plt.subplots(figsize=(20, 5))\n",
    "    ax.set_title('all predictor signals'.format(\"seaborn\"), color='C1')   \n",
    "    for i in range(1):\n",
    "        ax.plot([j for j in range(1, u_num-1)], [X[j, i] for j in range(1, u_num-1)], linewidth=0.5)\n",
    "    for i in range(-1,-6, -1):\n",
    "        ax.plot([j for j in range(1, u_num-1)], [X[j, i] for j in range(1, u_num-1)], linewidth=0.5)\n",
    "    ax.axvline(x=forget, ls = \"--\", c = \"yellow\")\n",
    "    ax.axvline(x=u_num - cutout, ls = \"--\", c = \"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EchoStateDeep():\n",
    "    \"\"\"\n",
    "    A Echo State Neural Network that can obtain deep structure\n",
    "    :param self.leak: The leaky rate of ESN in [0,1]\n",
    "    :param self.resvoir_num: number of layers of reservoir\n",
    "    :param self.encorporate: whether concatenate input with states when making prediction\n",
    "    :param x_dim: square root of state dimension (side length of W matrix), x_dim * x_dim = state dimension\n",
    "    :param u_dim: dimension of the direct input signal\n",
    "    :param W_list: list of W(s) in each reservoir layer \n",
    "    :param W_trans_list: list of reservoir-to-reservoir transition matrix\n",
    "    :param W_in: the input matrix in the preceding first layer of reservoir\n",
    "    :param model: the linear regression model of the Echo State Network object\n",
    "    \n",
    "    :return: a Deep Echo State Network Object\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, leak, resvoir_num, encorporate = True):\n",
    "        \"\"\"\n",
    "        Initialize ESN object leaky rate, reservoir depth, and predictor input concatenation indication\n",
    "        \n",
    "        :param leak: leaky rate, used to initialize self.leak\n",
    "        :param resvoir_num, number of reservoir, used to initalize self.resvoir_num\n",
    "        :param encorporate, BOOL, True is concatenate, False is not concenate\n",
    "        \"\"\"\n",
    "        \n",
    "        self.leak = leak\n",
    "        self.resvoir_num = resvoir_num\n",
    "        self.encorporate = encorporate\n",
    "\n",
    "    def make_weights(self, x_dim, u_dim, connectivity, spectral_radius):\n",
    "        \"\"\"\n",
    "        Initialize deep ESN W_in, W(s), W_trans(s) matrices\n",
    "        \n",
    "        :param x_dim: used to initialize self.x_dim\n",
    "        :param u_dim: used to initialize self.u_dim\n",
    "        :param connectivity: connectivity of the W matrices\n",
    "        :param spectral_radius: spectral_radius of the W matrices\n",
    "        \"\"\"\n",
    "        # State dimension x_dim * x_dim\n",
    "        self.x_dim = x_dim\n",
    "        \n",
    "        # Input dimension\n",
    "        self.u_dim = u_dim\n",
    "        \n",
    "        # Making inner transition sparse matrix self.W     \n",
    "        self.W_list = []\n",
    "        for i in range(self.resvoir_num):\n",
    "            nans = np.random.randint(0, int(1/connectivity)-1 , size=(x_dim, x_dim))\n",
    "            W = np.random.uniform(-0.4, 0.4, x_dim*x_dim).reshape([x_dim, x_dim])\n",
    "            W = np.where(nans, np.nan, W)\n",
    "            W = np.nan_to_num(W)\n",
    "            E, _ = np.linalg.eig(W)\n",
    "            e_max = np.max(np.abs(E))\n",
    "            W /= np.abs(e_max)/spectral_radius   \n",
    "            self.W_list.append(W)\n",
    "\n",
    "        # making reservoir to reservoir transition matrix [self.W_trans]\n",
    "        self.W_trans_list = []\n",
    "        for i in range(self.resvoir_num-1):\n",
    "            W_trans = np.random.uniform(-1, 1, x_dim*x_dim).reshape([x_dim, x_dim])\n",
    "            self.W_trans_list.append(W_trans) \n",
    "\n",
    "\n",
    "        # Making input matrix self.W_in\n",
    "        self.W_in = np.random.uniform(-1, 1, self.x_dim*self.u_dim).reshape([self.x_dim, self.u_dim])\n",
    "        self.W_in = self.W_in / (np.linalg.svd(self.W_in)[1].tolist()[0]*1.2)\n",
    "    \n",
    "    def make_state(self, u_input):\n",
    "        # X_lists is a list of reservoir list (X_list)\n",
    "        # X_list is a list of reservoir states\n",
    "        x_lists = []\n",
    "        \n",
    "        # Give the initial 2 empty states, \n",
    "        # 1st one for previous state, compensating leaky operator\n",
    "        # 2nd one for current state, integrated to compute next state \n",
    "        x_list = [np.zeros([self.x_dim]), np.zeros([self.x_dim])]\n",
    "        \n",
    "        u_num = u_input.shape[0]\n",
    "        # making 1st reservoir inner states\n",
    "        for i in range(u_num-1): # Do not use the last input to predict an extra state.\n",
    "                                 # State and input ending same length\n",
    "            x_next = (1-leak)*x_list[-2] + leak * np.tanh(np.matmul(self.W_in, u_input[i]) + \n",
    "                                                          np.matmul(self.W_list[0], x_list[-1] ) +  \n",
    "                                                          np.random.rand(self.x_dim))\n",
    "            x_list.append(x_next)\n",
    "        x_lists.append(x_list)\n",
    "\n",
    "        # 2-last reservoirs\n",
    "        for res in range(self.resvoir_num-1):\n",
    "            x_list = [np.zeros([self.x_dim]), np.zeros([self.x_dim])]\n",
    "            for i in range(u_num-1):  # Do not use the last input to predict an extra state.\n",
    "                                      # State and input ending same length\n",
    "                x_next = (1-leak)*x_list[-2] + leak * np.tanh(np.matmul(self.W_trans_list[res], x_lists[res][i+2] + # previous reservoir's states as input\n",
    "                                                              np.matmul(self.W_list[res], x_list[-1] ) +            # last state of this reservoir\n",
    "                                                              np.random.rand(self.x_dim)/10 ))                       # perturbing gaussian noise, std=0.1\n",
    "                x_list.append(x_next)\n",
    "            x_lists.append(x_list)\n",
    "            \n",
    "        return x_lists\n",
    "            \n",
    "            \n",
    "    def make_X(self, u_input):\n",
    "        \"\"\"\n",
    "        Computing all states (concatenated with inpulse signal), X, that can \n",
    "        be generated by u_input without further input\n",
    "        \n",
    "        :param u_input: the direct inpulse signal\n",
    "        \n",
    "        :return the finalized predictor variable X\n",
    "        \"\"\"\n",
    "        x_lists = self.make_state(u_input)\n",
    "        \n",
    "        # eliminate first null state as it is used for leak operator, no input signal aligns with it\n",
    "        X = np.array(x_lists[-1][1:]).reshape(u_num, self.x_dim) \n",
    "        \n",
    "        # if also encorporate input signal as part of predictor data X\n",
    "        if self.encorporate:\n",
    "            X = np.concatenate([X, u_input], axis=1) \n",
    "        \n",
    "        if not self.encorporate:\n",
    "            X = X\n",
    "        \n",
    "        # eliminate last state because there is no u signal for it\n",
    "        X = X[:-1, :]\n",
    "        \n",
    "        return X\n",
    "                                                              \n",
    "    def train(self, u_input, Y_target, forget, cv_start=0, cv_end=1, cv_step=0.01, val_cut=0.25):\n",
    "        \"\"\"\n",
    "        Train the model\n",
    "        \n",
    "        :param u_input: inpulse signal\n",
    "        :param Y_target: target signal\n",
    "        :param forget: the index after which the X is used for regression, due to memory vanishing\n",
    "        :param cv_start: the starting alpha value for validation\n",
    "        :param cv_end: the ending alpha value for validation\n",
    "        :param cv_step: the step wise increase of alpha for validation\n",
    "        :param val_cut: training and validation split ratio\n",
    "        \n",
    "        :return self.model\n",
    "        \"\"\"                                              \n",
    "        # Make training predicting variable\n",
    "        regress_X = self.make_X(u_input)[forget:]\n",
    "        regress_Y = Y_target[forget:]\n",
    "                       \n",
    "        train_size = int(regress_X.shape[0]*(1-val_cut))\n",
    "        Xtrain, Xval = np.split(regress_X, [train_size])\n",
    "        Ytrain, Yval = np.split(regress_Y, [train_size])\n",
    "                                       \n",
    "        # validation\n",
    "        alpha = cv_start\n",
    "        mse = {}\n",
    "        while alpha <= cv_end:\n",
    "\n",
    "            # Conducting linear regression\n",
    "            reg = Ridge(alpha).fit(Xtrain, Ytrain)\n",
    "\n",
    "            # Making prediction\n",
    "            Yval_hat = reg.predict(Xval)\n",
    "            alpha += cv_step\n",
    "\n",
    "            loss = np.mean(np.multiply(   (Yval - Yval_hat), (Yval - Yval_hat)))\n",
    "            mse[alpha] = loss\n",
    "\n",
    "        best_mse = min(list(mse.values()))\n",
    "        best_alpha = list(mse.keys())[list(mse.values()).index(best_mse)]\n",
    "        \n",
    "        self.model = Ridge(best_alpha).fit(regress_X, regress_Y)\n",
    "        \n",
    "    def predict_with_true_input(self, u_input, Y_true, predict_index):\n",
    "        \"\"\"\n",
    "        Make prediction with consecutive true input, State NOT depending on output feedback\n",
    "        \n",
    "        :param u_input: input signal [--forget--(forget_index)--predict]\n",
    "        :param Y_true: target signal [--forget--(forget_index)--predict]\n",
    "        :param predict_index: before which the state is built to bulge memory.\n",
    "                              aka, after which is used for prediction\n",
    "        \"\"\"\n",
    "        X = self.make_X(u_input)\n",
    "        X_pred = X[predict_index:]\n",
    "        Y_pred = Y_true[predict_index:]\n",
    "\n",
    "        Y_hat = self.model.predict(X_pred)\n",
    "        pred_mse = np.mean(np.multiply(   (Y_pred - Y_hat), (Y_pred - Y_hat)))\n",
    "        \n",
    "        print(\"Model prediction mse:\", pred_mse)\n",
    "        print(\"Model prediction average error\", math.sqrt(pred_mse))\n",
    "        \n",
    "    def extend_by_one(self, u_input_last, x_all_resvoir):\n",
    "        \"\"\"\n",
    "        Extend reservoir state and predictor X by 1 \n",
    "        \n",
    "        :param u_input_last: the last calculated input signal\n",
    "        :param x_all_resvoir: a LIST of all previous reservoir states ARRAY, \n",
    "            indexing: x_all_resvoir[i] is the (i+1)th reservoir; x_all_resvoir[][-j,:], the last jth state of the reservoir\n",
    "        \n",
    "        :return x_all_reservoir: the extended all reservoir state list\n",
    "        :return X: the predictor for the next time step\n",
    "        \"\"\"\n",
    "        # 1st reservoir: x_all_resvoir[0] indicate the first reservoir\n",
    "        x_one_next = (1-leak)*x_all_resvoir[0][-2,:] + leak * np.tanh(np.matmul(self.W_in, u_input_last) + # current input as input signal\n",
    "                                                                   np.matmul(self.W_list[0], x_all_resvoir[0][-1,:]) + # last state of this reservoir\n",
    "                                                                   np.random.rand(self.x_dim)/10) # perturbing gaussian noise, std=0.1\n",
    "        x_all_resvoir[0].append(x_one_next)\n",
    "\n",
    "        # 2-last reservoirs\n",
    "        for i in range(self.resvoir_num-1):                # indexing: x_all_resvoir[i+1] indicate the current reservoir\n",
    "            x_next = (1-leak)*x_all_resvoir[i+1][-2,:] + \\\n",
    "                      leak * np.tanh(np.matmul(self.W_trans_list[res], x_all_resvoir[i][-1,:]) + # previous reservoir's states as input\n",
    "                      np.matmul(self.W_list[res], x_all_resvoir[i+1][-1,:] ) +            # last state of this reservoir\n",
    "                      np.random.rand(self.x_dim)/10 )                       # perturbing gaussian noise, std=0.1\n",
    "            x_all_resvoir[i+1].append(x_next)\n",
    "            \n",
    "        \n",
    "        \n",
    "        # if also encorporate input signal as part of predictor data X\n",
    "        if self.encorporate:\n",
    "            X = np.concatenate([x_all_resvoir[-1][-1,:], u_input], axis=1)\n",
    "        \n",
    "        if not self.encorporate:\n",
    "            X = x_all_resvoir[-1][-1,:]\n",
    "        \n",
    "        return x_all_resvoir, X\n",
    "        \n",
    "    def make_one_prediction(self, X):\n",
    "        \"\"\"\n",
    "        Predict one step given the X predictor\n",
    "        \n",
    "        :param X, the one more step predictor variable\n",
    "        \"\"\"\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "        \n",
    "    def predict_without_true_input(self, u_input, Y_true, predict_index):\n",
    "        \"\"\"\n",
    "        :param u_input: input signal [--forget--(forget_index)---null---]\n",
    "        :param Y_true: target signal [--forget--(forget_index)--predict-]\n",
    "        State NOT depending on output feedback\n",
    "        \"\"\"\n",
    "        Y_pred = u_input\n",
    "        \n",
    "        x_all_resvoir = self.make_state(u_input)\n",
    "        \n",
    "        while Y_pred.shape != Y_true.shape:\n",
    "            \n",
    "            x_all_resvoir, X= self.extend_by_one(x_all_resvoir)\n",
    "            Y_u_next = self.make_one_prediction(X)\n",
    "            Y_pred = np.concatenate([])\n",
    "            \n",
    "        # showing training error, \n",
    "        regrpred_Y = reg.predict(regress_X)\n",
    "        train_mse = np.mean(np.multiply(   (regrpred_Y - regress_Y), (regrpred_Y - regress_Y)))\n",
    "        pred_mse = np.mean(np.multiply(   (predhat_Y - predict_Y), (predhat_Y - predict_Y)))\n",
    "\n",
    "        print(\"model ridge coefficient:\", best_alpha)\n",
    "        print(\"Model training mse:\", train_mse)\n",
    "        print(\"Model validation mse:\", best_mse)\n",
    "        print(\"Model prediction mse:\", pred_mse)\n",
    "        print(\"Model prediction average error\", math.sqrt(pred_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 The Constant $\\mu$ modeling\n",
    "## 1.1 Choosing the appropriate data set\n",
    "Due to the fact that we're assuming that $\\mu$ as a constant (process), we're avoiding the time period where \"black swan\" occurs. The following segment of SPX500 data looks like a good fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contains 1469 seqential observations.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./SPX500.TXT\", names = [\"date\", \"open\",\"high\", \"low\", \"close\"], delimiter=\" \", index_col =False)\n",
    "close_p = data[\"close\"].values\n",
    "\n",
    "start = close_p.tolist().index(np.min(close_p))\n",
    "data = close_p[start:]\n",
    "data\n",
    "\n",
    "mpl.style.use(\"seaborn\")\n",
    "plt.plot(data)\n",
    "print(\"The data contains {} seqential observations.\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Define the geometric brownian motion\n",
    "\n",
    "**Parameter:**\n",
    "$r_t$: stock prices at time t\n",
    "\n",
    "By definition, the process defined by $dS_t = \\mu S_t dt + \\sqrt{v_t}S_td\\tilde W_t$ is a geometric brownian motion. The property is: $S(t+\\Delta t) - S(t) \\sim N(\\mu, \\Delta t v_t)$. \n",
    "\n",
    "We first do frequensist inference for the constant drift $\\mu$. To do this, we add up all the segment differences $\\sum_n^{1469-1}\\Delta r_{t_n} = \\sum_n^{1468}(r_{t_n}-r_{t_{n+1}})$. ( The reason why it is 1469 is that we have 1469 data points, therefore we have 1468 segments):\n",
    "\n",
    "\\begin{aligned}\n",
    "S(t+\\Delta t) - S(t) &\\sim N(\\mu, \\Delta t v_t) \\\\\n",
    "\\frac{1}{1468}\\sum_n^{1468}\\Delta r_{t_n} &\\sim N(\\mu, \\frac{\\Delta t \\sum_n v_{t_n}}{1468}) \n",
    "\\end{aligned}\n",
    "\n",
    "Therefore, from frequensist view, that $\\frac{1}{1468}\\sum_n^{1468}\\Delta r_{t_n}$ is an unbiased and efficient estimator for $\\mu$. We might be aware of the fact that this distribution has parameter $\\Delta t$. How big a difference exactly is this difference in time? It doesn't matter in this modeling process (in fact it is a trading day) as the volatility $v_t$ can be adaptively scaled up or down to fit the curve. Therefore, it suffice to regard $\\Delta t = 1$, leading to:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\frac{1}{1468}\\sum_n^{1468}\\Delta r_{t_n} &\\sim N(\\mu, \\frac{\\sum_n v_{t_n}}{1468}) \n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average increase in stock price is : 0.8145231607629428\n"
     ]
    }
   ],
   "source": [
    "# Inferencing the value of mu\n",
    "avg_mu = 0\n",
    "cutout = 200\n",
    "\n",
    "length = data.shape[0]\n",
    "for i in range(length-1-cutout):\n",
    "    avg_mu += data[i+1] - data[i]\n",
    "avg_mu /= (data.shape[0]-1)\n",
    "\n",
    "print(\"average increase in stock price is : {}\".format(avg_mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Constructing required dataset\n",
    "\n",
    "We're making the data input and the fitting target. \n",
    "\n",
    "\n",
    "First, we transform stock price data into matrix. Notice that as we're feeding in vector, we can feed in consecutive days (e.g. 5 days) as one sample. This will manually bring longer past memory into the Echo State Network and therefore better deal with long-term memory fitting.\n",
    "\n",
    "Second, as the target output is a the return at the next time step, which follows the distribution $r_{t+1} \\sim N(r_t+\\mu, v_{t+1})$, where $v_{t+1}$ is the parameter we're interested in, the key is to maximize the log likelihood of $r_{t+1}$ by nicely predicting $v_{t+1}$:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\ell(r_{t+1}; r_{t}, \\mu, v_{t+1}) &= \\log(\\frac{1}{\\sqrt{2\\pi v_{t+1}}}\\mathrm{ Exp}\\{-\\frac{(r_{t+1}-r_{t}-\\mu)^2}{2v_{t+1}}\\})\\\\\n",
    "    &= -\\frac{1}{2}\\log{(2\\pi v_{t+1})}-\\frac{(r_{t+1}-r_{t}-\\mu)^2}{2v_{t+1}}\\\\\n",
    "    &= -\\frac{1}{2}\\log{(2\\pi)}-\\frac{1}{2}\\log{v_{t+1}}-\\frac{(r_{t+1}-r_{t}-\\mu)^2}{2v_{t+1}}\n",
    "\\end{aligned}\n",
    "\n",
    "It is easy to picture that there the function $f(v_{t+1}) = \\ell(r_{t+1}; r_{t}, \\mu, v_{t+1})$ is a convex function. Therefore, to get the global maximum, we merely need to use calculate derivative to get local maxima (\\bf NOTE: the minima doesn't exist as the pdf can be arbiturarily flat so that the likelihood will go to 0) :\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\frac{d\\ell(r_{t+1}; r_{t}, \\mu, v_{t+1})}{dv_{t+1}} &= -\\frac{1}{2v_{t+1}} + \\frac{(r_{t+1}-r_{t}-\\mu)^2}{2v_{t+1}^2} = 0\\\\\n",
    "    v_{t+1} &= (r_{t+1}-r_{t}-\\mu)^2 \\geq 0\n",
    "\\end{aligned}\n",
    "\n",
    "In order to make sure that during prediction phase, the model won't predict a negative variance, we'd take the square root of the target variance and let ESN to predict the standard deviation instead. After squareing the standard deviation, we'll end up with a non negative variance:\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\sigma_{t} &= \\sqrt{r_t} = |r_{t+1}-r_{t}-\\mu|\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "The calculated value is the target output sequence\n",
    "\n",
    "**NOTE: target should be 1 sample less than input, as for t at last, there is nothing to be predicted, aka, lack of one future prediction target value for the last input sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input: (#samples, #dimensions): (1270, 200)\n",
      "shape of target: (#samples, #dimensions): (1269, 1)\n"
     ]
    }
   ],
   "source": [
    "# Making input states\n",
    "u_dim = 200\n",
    "u_num = length + 1 - u_dim\n",
    "\n",
    "u = []\n",
    "for i in range(u_num):\n",
    "    ui = []\n",
    "    for j in range(u_dim):\n",
    "        ui.append(data[i+j])\n",
    "    ui = np.array(ui)\n",
    "    u.append(ui)\n",
    "u = np.array(u).reshape(u_num, u_dim)\n",
    "\n",
    "# Making target states\n",
    "r_t_1 = data[u_dim:]\n",
    "r_t_0 = data[u_dim-1:-1]\n",
    "target = np.reshape(np.abs(r_t_1 - r_t_0 - avg_mu), [-1,1])\n",
    "\n",
    "print(\"shape of input: (#samples, #dimensions): {}\".format(u.shape))\n",
    "print(\"shape of target: (#samples, #dimensions): {}\".format(target.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Plug in data and start the training\n",
    "\n",
    "We need to normalize the input data to make it not too big that dominates the previous tanh state which ranges between -1 and 1. Below are all extra needed parameters needed for training the Echo State Network\n",
    "\n",
    "### Parameters\n",
    "**u_dim:** (above) dimension of input data (how many consecutive days)  \n",
    "**u_num:** (above) length of input data (how many days of observations)  \n",
    "**u_mag:** scale of magnitude of entries of input data (np.max(u)=u_mag)  \n",
    "**x_dim:** dimension of inner state  \n",
    "**connectivity:** level of connectivity, range between $[0,1]$  \n",
    "**spectral_radius:** spectral radius of $\\mathbf{W}_{\\text{r}}$   \n",
    "**cutout:** (above) number of observation at the end for prediction   \n",
    "**forget:** number of observation at the begining not participating model training   \n",
    "**cv_start:** the lower bound of alpha coefficient for ridge regression validation   \n",
    "**cv_end:** the upper bound of alpha coefficient for ridge regression validation   \n",
    "**cv_step:** the step of increment for alpha for ridge regression validation   \n",
    "**val_cut:** the proportion between validation set and training set, $=\\frac{\\# \\text{validation}}{\\# \\text{training}}$   \n",
    "**verbose:** for debug mode, =True will return information of model parameters   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction mse: 76.29955130873182\n",
      "Model prediction average error 8.734961437163408\n"
     ]
    }
   ],
   "source": [
    "x_dim = 35\n",
    "connectivity = 0.08\n",
    "spectral_radius = 0.6\n",
    "\n",
    "u_mag = 1\n",
    "\n",
    "\n",
    "leak = 1 # the bigger the more focused on present\n",
    "\n",
    "forget = 300\n",
    "\n",
    "cv_start = 0\n",
    "cv_end = 1\n",
    "cv_step = 0.01\n",
    "val_cut = 0.2\n",
    "\n",
    "verbose = False\n",
    "\n",
    "\n",
    "u = u/np.max(u) * u_mag\n",
    "\n",
    "\n",
    "\n",
    "ESN = EchoStateDeep(leak, resvoir_num=1, encorporate = True)\n",
    "ESN.make_weights(x_dim, u_dim, connectivity, spectral_radius)\n",
    "ESN.train(u, target, forget)\n",
    "ESN.predict_with_true_input(u, target, forget)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pred_std = EchoStateDeepTest(x_dim, connectivity, spectral_radius,\n",
    "                u_num, u_dim, u_mag,\n",
    "                  leak, cutout, forget, \n",
    "                  cv_start, cv_end, cv_step, val_cut, verbose, u, target, resvoir_num=1)\n",
    "\n",
    "pred = [data[-cutout-1]]\n",
    "\n",
    "for i in range(cutout):\n",
    "    pred.append(pred[-1]+np.random.normal(avg_mu, pred_std[i]))\n",
    "\n",
    "pred = np.array(pred[1:])\n",
    "print(np.mean( np.power(pred-data[-cutout:], 2)  ))\n",
    "\n",
    "mpl.style.use(\"seaborn\")\n",
    "fig, ax = plt.subplots(figsize=(20, 5))\n",
    "ax.set_title('all predictor signals'.format(\"seaborn\"), color='C1')\n",
    "ax.plot([j for j in range(cutout)], [data[-cutout:][j] for j in range(cutout)])\n",
    "ax.plot([j for j in range(cutout)], [pred[j] for j in range(cutout)], \"--\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
